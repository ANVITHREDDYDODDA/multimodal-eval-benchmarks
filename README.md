# multimodal-eval-benchmarks
Work-in-progress benchmark &amp; evaluation suite for multimodal/generative media modelsâ€”task sets, scoring metrics, and human preference rubrics (started Dec 18, 2025).

## Progress log
- Day 1: defined benchmark scope + failure taxonomy; drafted v0.1 tasks; created human rubric outline
- Next 48 hours: expand to v0.2 (harder prompts), add rater QA examples, and add a one-page reporting template.

- ## Repo map
- Tasks: `tasks/task_catalog.md`
- Prompts: `tasks/prompts_v0.1.md`
- Human rubric: `rubric/human_rubric.md`
- Evaluation protocol: `eval/evaluation_protocol.md`
- Metrics plan: `metrics/metrics_plan.md`
- Dry run report: reporting/run_report_2025-12-18.md
- Dry run scoring sheet: reporting/scoring_sheet_example.csv

## What a run produces
- Overall + per-task win-rates (pairwise preference)
- Rubric dimension scores (adherence, temporal, identity, realism, edit precision)
- Failure-tag distribution + top examples (for iteration)

## Changelog
- 2025-12-18: v0.1 repo initialized; tasks, rubric, protocol, metrics plan, prompt set added

